"""Exploitative agent with opponent modeling and episodic memory."""

import json
import os
import re
from typing import List, Optional, Dict
from collections import defaultdict
from dotenv import load_dotenv
from anthropic import Anthropic
from .base_agent import BaseAgent
from .llm_logger import LLMLogger
from kuhn_poker.state import GameState
from kuhn_poker.gto_strategy import GTOStrategy

# Load environment variables from .env file
load_dotenv()


class ExploitativeAgent(BaseAgent):
    """Agent that uses opponent modeling to exploit patterns.

    Inspired by CoALA cognitive architecture:
    - Working Memory: Current game state
    - Episodic Memory: Opponent action history
    - Procedural Memory: Exploitation rules (from prompt)

    Uses Claude API to analyze patterns and choose exploitative actions.
    """

    def __init__(self, name: str = "Exploitative", api_key: Optional[str] = None):
        """Initialize the exploitative agent.

        Args:
            name: Agent name
            api_key: Anthropic API key (if not provided, loads from ANTHROPIC_API_KEY env var)
        """
        super().__init__(name)
        self.gto_strategy = GTOStrategy()

        # Initialize LLM logger
        self.logger = LLMLogger(agent_name=name)

        # Episodic memory: Track opponent actions
        self.opponent_actions = []  # List of (info_set, action) tuples
        self.opponent_stats = {
            'fold_to_bet': 0,
            'call_bet': 0,
            'total_facing_bet': 0,
            'voluntary_bet': 0,
            'total_opportunities': 0,
        }

        # Get API key from parameter or environment variable
        if not api_key:
            api_key = os.getenv('ANTHROPIC_API_KEY')
        if not api_key:
            raise ValueError(
                "API key required. Provide via api_key parameter or set ANTHROPIC_API_KEY environment variable."
            )
        self.client = Anthropic(api_key=api_key)

    def choose_action(self, state: GameState, legal_actions: List[str], player_position: int) -> str:
        """Choose exploitative action based on opponent model using Claude API.

        Args:
            state: Current game state
            legal_actions: List of legal actions
            player_position: This agent's position (0 or 1)

        Returns:
            Exploitative action
        """
        return self._choose_action_with_llm(state, legal_actions, player_position)

    def _choose_action_with_llm(self, state: GameState, legal_actions: List[str], player_position: int) -> str:
        """Choose action using Claude API with opponent modeling.

        Args:
            state: Current game state
            legal_actions: List of legal actions
            player_position: This agent's position

        Returns:
            Action chosen by LLM
        """
        card = state.cards[player_position]
        info_set = state.get_info_set(player_position)
        history = state.get_betting_string()

        # Get GTO baseline action
        gto_strategy = self.gto_strategy.get_strategy(info_set)
        gto_action = max(gto_strategy.items(), key=lambda x: x[1])[0]

        # Format opponent statistics
        stats_str = self._format_opponent_stats()

        # Format recent opponent actions
        recent_actions = self._format_recent_actions()

        prompt = f"""You are an expert Kuhn Poker player who models opponents to exploit their weaknesses.

YOUR GOAL: Analyze opponent patterns and choose an action that exploits them.

WORKING MEMORY (current situation):
- Your card: {card}
- Betting history this hand: {history if history else "You act first"}
- Legal actions: {legal_actions}
- Current pot: {state.pot} chips

EPISODIC MEMORY (opponent patterns):
{stats_str}

RECENT OPPONENT ACTIONS:
{recent_actions}

PROCEDURAL KNOWLEDGE (exploitation strategies):
- If opponent folds >60% to bets: Bluff more with Jack (they fold too much)
- If opponent bets >40% voluntarily: Call lighter with Queen (they bluff too much)
- If opponent calls >70% of bets: Only bet with King/Queen (they call too much)
- If no clear pattern yet (<20 hands): Default to GTO strategy

GTO BASELINE (if unsure):
The optimal play in this situation would be: {gto_action}
(Use this if no exploitable pattern is detected)

TASK: Analyze opponent patterns and choose the action that best exploits them.
If no clear pattern exists yet, default to GTO.

Output ONLY valid JSON (no other text):
{{
  "action": "BET",
  "reasoning": "Opponent folds 68% to bets, exploiting with Jack bluff",
  "exploiting_pattern": "high_fold_frequency",
  "confidence": 0.8
}}

Your response:"""

        try:
            model = os.getenv('ANTHROPIC_MODEL', 'claude-3-5-sonnet-20241022')
            response = self.client.messages.create(
                model=model,
                max_tokens=200,
                temperature=0.7,  # Balanced temperature
                messages=[{"role": "user", "content": prompt}]
            )

            response_text = response.content[0].text.strip()

            # Log the conversation
            self.logger.log_conversation(
                prompt=prompt,
                response=response_text,
                model=model,
                metadata={
                    "card": card,
                    "info_set": info_set,
                    "history": history,
                    "legal_actions": legal_actions,
                    "pot": state.pot,
                    "temperature": 0.7,
                    "max_tokens": 200
                }
            )

            # Try to parse JSON - search for JSON object pattern
            try:
                # Find JSON object in response (handles markdown fences, extra text, etc.)
                match = re.search(r'\{.*?\}', response_text, re.DOTALL)
                if match:
                    result = json.loads(match.group(0))
                    action = result.get('action', '').upper()

                    # Validate action
                    if action in legal_actions:
                        return action
            except (json.JSONDecodeError, AttributeError):
                pass

            # Fallback to GTO if parsing fails
            print(f"Warning: Could not parse LLM response, using GTO fallback")
            return self._gto_fallback(info_set, legal_actions)

        except Exception as e:
            # Log the error
            self.logger.log_error(
                error=e,
                prompt=prompt,
                metadata={
                    "card": card,
                    "info_set": info_set,
                    "history": history,
                    "legal_actions": legal_actions
                }
            )
            print(f"Warning: LLM call failed ({e}), using GTO fallback")
            return self._gto_fallback(info_set, legal_actions)

    def _gto_fallback(self, info_set: str, legal_actions: List[str]) -> str:
        """Fallback to GTO strategy when LLM fails.

        Args:
            info_set: Current information set
            legal_actions: List of legal actions

        Returns:
            GTO action
        """
        gto_strategy = self.gto_strategy.get_strategy(info_set)
        gto_action = max(gto_strategy.items(), key=lambda x: x[1])[0]
        if gto_action in legal_actions:
            return gto_action
        return legal_actions[0]

    def _format_opponent_stats(self) -> str:
        """Format opponent statistics for the LLM prompt.

        Returns:
            Formatted statistics string
        """
        if self.opponent_stats['total_opportunities'] == 0:
            return "No opponent data yet (first few hands)"

        fold_rate = self._get_fold_rate()
        call_rate = self._get_call_rate()
        bet_rate = self._get_bet_rate()

        hands_observed = self.opponent_stats['total_opportunities']

        return f"""Opponent Statistics (last {hands_observed} decision points):
- Fold to bet: {fold_rate*100:.1f}% ({self.opponent_stats['fold_to_bet']}/{self.opponent_stats['total_facing_bet']} times)
- Call when facing bet: {call_rate*100:.1f}% ({self.opponent_stats['call_bet']}/{self.opponent_stats['total_facing_bet']} times)
- Voluntary bet rate: {bet_rate*100:.1f}% ({self.opponent_stats['voluntary_bet']}/{hands_observed} opportunities)"""

    def _format_recent_actions(self) -> str:
        """Format recent opponent actions.

        Returns:
            String describing recent actions
        """
        if not self.opponent_actions:
            return "No actions observed yet"

        recent = self.opponent_actions[-10:]  # Last 10 actions
        action_list = [f"{info_set} â†’ {action}" for info_set, action in recent]

        return "Last 10 opponent decisions:\n" + "\n".join(action_list)

    def _get_fold_rate(self) -> float:
        """Calculate opponent's fold rate when facing bets."""
        if self.opponent_stats['total_facing_bet'] == 0:
            return 0.0
        return self.opponent_stats['fold_to_bet'] / self.opponent_stats['total_facing_bet']

    def _get_call_rate(self) -> float:
        """Calculate opponent's call rate when facing bets."""
        if self.opponent_stats['total_facing_bet'] == 0:
            return 0.0
        return self.opponent_stats['call_bet'] / self.opponent_stats['total_facing_bet']

    def _get_bet_rate(self) -> float:
        """Calculate opponent's voluntary bet rate."""
        if self.opponent_stats['total_opportunities'] == 0:
            return 0.0
        return self.opponent_stats['voluntary_bet'] / self.opponent_stats['total_opportunities']

    def observe_opponent_action(self, state: GameState, action: str, player_position: int) -> None:
        """Track opponent action for modeling.

        Args:
            state: State when opponent acted
            action: Opponent's action
            player_position: This agent's position
        """
        opponent_position = 1 - player_position
        opponent_info_set = state.get_info_set(opponent_position)

        # Record action
        self.opponent_actions.append((opponent_info_set, action))

        # Update statistics
        self.opponent_stats['total_opportunities'] += 1

        # Check if facing a bet
        if 'CALL' in [action] or 'FOLD' in [action]:
            self.opponent_stats['total_facing_bet'] += 1
            if action == 'FOLD':
                self.opponent_stats['fold_to_bet'] += 1
            elif action == 'CALL':
                self.opponent_stats['call_bet'] += 1

        # Check for voluntary bets
        if action == 'BET':
            self.opponent_stats['voluntary_bet'] += 1

        # Keep only last 100 actions in memory
        if len(self.opponent_actions) > 100:
            self.opponent_actions.pop(0)

    def reset(self) -> None:
        """Reset opponent model."""
        self.opponent_actions = []
        self.opponent_stats = {
            'fold_to_bet': 0,
            'call_bet': 0,
            'total_facing_bet': 0,
            'voluntary_bet': 0,
            'total_opportunities': 0,
        }
